{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "060d8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import soundfile as sf\n",
    "import jiwer\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4da39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EXTRA_DATA = True\n",
    "CACHE_DIR = \".\"\n",
    "DATA_SUBDIR = \"datasets\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "VAL_BATCH  = 8\n",
    "EPOCHS = 30\n",
    "MAX_TARGET_LEN = 200\n",
    "AUDIO_PAD_LEN = 2754            # ~10s\n",
    "FFT_LENGTH = 256\n",
    "HOP = 80\n",
    "WIN = 200\n",
    "FEAT_DIM = FFT_LENGTH // 2 + 1  # 129 for fft_length=256\n",
    "START_TOKEN_IDX = 2             # '<'\n",
    "END_TOKEN_IDX   = 3             # '>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24643e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-io (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-io\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46d988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (10.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "Installing collected packages: tensorflow\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.19.0\n",
      "    Uninstalling tensorflow-2.19.0:\n",
      "      Successfully uninstalled tensorflow-2.19.0\n",
      "Successfully installed tensorflow-2.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb94c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mufeez\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5739e",
   "metadata": {},
   "source": [
    "# Download the dataset & extract the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train100_dir = f\"./{DATA_SUBDIR}/train-clean-100/LibriSpeech/train-clean-100\"\n",
    "train360_url = \"https://www.openslr.org/resources/12/train-clean-360.tar.gz\"\n",
    "train500_url = \"https://www.openslr.org/resources/12/train-other-500.tar.gz\"\n",
    "\n",
    "\n",
    "if USE_EXTRA_DATA:\n",
    "    keras.utils.get_file(fname=\"train-clean-360.tar.gz\", origin=train360_url,\n",
    "                         extract=True, cache_dir=CACHE_DIR, cache_subdir=DATA_SUBDIR)\n",
    "    keras.utils.get_file(fname=\"train-other-500.tar.gz\", origin=train500_url,\n",
    "                         extract=True, cache_dir=CACHE_DIR, cache_subdir=DATA_SUBDIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35e3b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folders: ['./datasets\\\\train-clean-100_extracted\\\\LibriSpeech\\\\train-clean-100', './datasets\\\\train-clean-360_extracted\\\\LibriSpeech\\\\train-clean-360', './datasets\\\\train-other-500_extracted\\\\LibriSpeech\\\\train-other-500']\n"
     ]
    }
   ],
   "source": [
    "candidates = glob(f\"./{DATA_SUBDIR}/**/LibriSpeech/**/train-*\", recursive=True)\n",
    "all_data_dirs = []\n",
    "if os.path.isdir(train100_dir):\n",
    "    all_data_dirs.append(train100_dir)\n",
    "for c in candidates:\n",
    "    if c not in all_data_dirs:\n",
    "        all_data_dirs.append(c)\n",
    "print(\"Using data folders:\", all_data_dirs)\n",
    "\n",
    "# ---------------- Transcripts -----------\n",
    "pattern_wav_name = re.compile(r\"([^/\\\\\\.]+)\")\n",
    "id_to_text = {}\n",
    "for folder in all_data_dirs:\n",
    "    trans_files = glob(f\"{folder}/**/*.trans.txt\", recursive=True)\n",
    "    for trans_path in trans_files:\n",
    "        with open(trans_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    utt_id, text = parts\n",
    "                    id_to_text[utt_id] = text.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc53976",
   "metadata": {},
   "source": [
    "# 3. MAP AUDIO FILES TO TRANSCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b272d502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLAC files: 281241\n",
      "Usable examples: 162743\n"
     ]
    }
   ],
   "source": [
    "def get_data(audio_files, id_to_text, maxlen=200):\n",
    "    data = []\n",
    "    for f in audio_files:\n",
    "        utt_id = os.path.splitext(os.path.basename(f))[0]\n",
    "        if utt_id in id_to_text and len(id_to_text[utt_id]) < maxlen:\n",
    "            data.append({\"audio\": f, \"text\": id_to_text[utt_id]})\n",
    "    return data\n",
    "\n",
    "flacs = glob(f\"{DATA_SUBDIR}/**/*.flac\", recursive=True)\n",
    "print(\"Total FLAC files:\", len(flacs))\n",
    "data = get_data(flacs, id_to_text, maxlen=MAX_TARGET_LEN)\n",
    "print(\"Usable examples:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237669a0",
   "metadata": {},
   "source": [
    "# 4. VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19c4424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 35\n"
     ]
    }
   ],
   "source": [
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=MAX_TARGET_LEN):\n",
    "        self.vocab = (\n",
    "            [\"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\", \"'\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.vocab)}\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab\n",
    "\n",
    "vectorizer = VectorizeChar(MAX_TARGET_LEN)\n",
    "idx_to_char = vectorizer.get_vocabulary()\n",
    "vocab_size = len(idx_to_char)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fa68c",
   "metadata": {},
   "source": [
    "# 5. AUDIO PROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fcbc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_flac_py(path):\n",
    "    path = path.numpy().decode(\"utf-8\")\n",
    "    audio, sr = sf.read(path, dtype=\"float32\")\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "    return audio\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.string)])\n",
    "def path_to_audio_graph(path):\n",
    "    audio = tf.py_function(func=decode_flac_py, inp=[path], Tout=tf.float32)\n",
    "    audio.set_shape([None])\n",
    "    stfts = tf.signal.stft(audio, frame_length=WIN, frame_step=HOP, fft_length=FFT_LENGTH)\n",
    "    x = tf.math.pow(tf.abs(stfts), 0.5)   # [T, FEAT_DIM]\n",
    "    x.set_shape([None, FEAT_DIM])\n",
    "    means = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "    stds = tf.math.reduce_std(x, axis=1, keepdims=True)\n",
    "    x = (x - means) / (stds + 1e-9)\n",
    "    paddings = tf.constant([[0, AUDIO_PAD_LEN], [0, 0]])\n",
    "    x = tf.pad(x, paddings)[:AUDIO_PAD_LEN, :]\n",
    "    x.set_shape([AUDIO_PAD_LEN, FEAT_DIM])\n",
    "    return x\n",
    "\n",
    "def create_tf_dataset(samples, bs=BATCH_SIZE, shuffle=True, augment=False):\n",
    "    flist = [ex[\"audio\"] for ex in samples]\n",
    "    texts = [vectorizer(ex[\"text\"]) for ex in samples]\n",
    "    ds_paths = tf.data.Dataset.from_tensor_slices(tf.constant(flist, dtype=tf.string))\n",
    "    ds_texts = tf.data.Dataset.from_tensor_slices(tf.constant(texts, dtype=tf.int32))\n",
    "    ds_audio = ds_paths.map(path_to_audio_graph, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = tf.data.Dataset.zip((ds_audio, ds_texts))\n",
    "    ds = ds.map(lambda a, t: {\"source\": a, \"target\": t}, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(4096, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(bs).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777755b9",
   "metadata": {},
   "source": [
    "# 6. TRAIN/VAL SPLIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f42514a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 146468 Val examples: 16275\n",
      "Batch 'source' shape: (8, 2754, 129)\n",
      "Batch 'target' shape: (8, 200)\n"
     ]
    }
   ],
   "source": [
    "split = int(len(data) * 0.9)\n",
    "train_data, test_data = data[:split], data[split:]\n",
    "print(\"Train examples:\", len(train_data), \"Val examples:\", len(test_data))\n",
    "\n",
    "train_ds = create_tf_dataset(train_data, bs=BATCH_SIZE, shuffle=True)\n",
    "val_ds = create_tf_dataset(test_data, bs=VAL_BATCH, shuffle=False)\n",
    "\n",
    "# Sanity shapes\n",
    "for b in train_ds.take(1):\n",
    "    print(\"Batch 'source' shape:\", b[\"source\"].shape)  # (B, AUDIO_PAD_LEN, FEAT_DIM)\n",
    "    print(\"Batch 'target' shape:\", b[\"target\"].shape)  # (B, MAX_TARGET_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785f78b",
   "metadata": {},
   "source": [
    "# Define the Transformer input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1272091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab, maxlen, num_hid):\n",
    "        super().__init__()\n",
    "        self.emb = layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        L = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        pos = self.pos_emb(tf.range(L))\n",
    "        return x + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e690a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = layers.Conv1D(num_hid, 3, strides=2, padding='same', activation='relu')\n",
    "        self.conv2 = layers.Conv1D(num_hid, 3, strides=2, padding='same', activation='relu')\n",
    "        self.conv3 = layers.Conv1D(num_hid, 3, strides=2, padding='same', activation='relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: [B, T, F]\n",
    "        return self.conv3(self.conv2(self.conv1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fedd5b",
   "metadata": {},
   "source": [
    "# Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42e36685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([layers.Dense(feed_forward_dim, activation='relu'), layers.Dense(embed_dim)])\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.do1 = layers.Dropout(rate)\n",
    "        self.do2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn = self.att(x, x)\n",
    "        attn = self.do1(attn, training=training)\n",
    "        out1 = self.ln1(x + attn)\n",
    "        ffn = self.ffn(out1)\n",
    "        ffn = self.do2(ffn, training=training)\n",
    "        return self.ln2(out1 + ffn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa456c2",
   "metadata": {},
   "source": [
    "# Transformer Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc6f8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.enc_att  = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.do1 = layers.Dropout(rate)\n",
    "        self.do2 = layers.Dropout(rate)\n",
    "        self.do3 = layers.Dropout(rate)\n",
    "        self.ffn = keras.Sequential([layers.Dense(feed_forward_dim, activation='relu'), layers.Dense(embed_dim)])\n",
    "\n",
    "    def _causal_mask(self, bs, L, dtype):\n",
    "        i = tf.range(L)[:, None]\n",
    "        j = tf.range(L)[None, :]\n",
    "        mask = tf.cast(i >= j, dtype)\n",
    "        mask = tf.reshape(mask, [1, L, L])\n",
    "        return tf.tile(mask, [bs, 1, 1])\n",
    "\n",
    "    def call(self, enc_out, target, training=False):\n",
    "        bs = tf.shape(target)[0]\n",
    "        L = tf.shape(target)[1]\n",
    "        causal = self._causal_mask(bs, L, tf.bool)\n",
    "        tgt_att = self.self_att(target, target, attention_mask=causal)\n",
    "        tgt_att = self.do1(tgt_att, training=training)\n",
    "        y = self.ln1(target + tgt_att)\n",
    "        enc_att = self.enc_att(y, enc_out)\n",
    "        enc_att = self.do2(enc_att, training=training)\n",
    "        y2 = self.ln2(y + enc_att)\n",
    "        ffn = self.ffn(y2)\n",
    "        ffn = self.do3(ffn, training=training)\n",
    "        return self.ln3(y2 + ffn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4448619",
   "metadata": {},
   "source": [
    "# Complete the Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f31003cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"custom\")\n",
    "class TransformerASR(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 num_hid=128, num_heads=4, num_feed_forward=512,\n",
    "                 source_maxlen=AUDIO_PAD_LEN, target_maxlen=MAX_TARGET_LEN,\n",
    "                 num_layers_enc=4, num_layers_dec=1, num_classes=vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid)\n",
    "        self.dec_input = TokenEmbedding(num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid)\n",
    "\n",
    "        enc_blocks = [TransformerEncoder(num_hid, num_heads, num_feed_forward, dropout_rate) for _ in range(num_layers_enc)]\n",
    "        self.encoder = keras.Sequential([self.enc_input] + enc_blocks)\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(self, f\"dec_layer_{i}\", TransformerDecoder(num_hid, num_heads, num_feed_forward, dropout_rate))\n",
    "\n",
    "        # classifier: if mixed_precision, use float32 for final logits via dtype\n",
    "        self.classifier = layers.Dense(num_classes, dtype='float32')\n",
    "\n",
    "    def decode(self, enc_out, target, training=False):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training=training)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        source, target = inputs\n",
    "        x = self.encoder(source, training=training)\n",
    "        y = self.decode(x, target, training=training)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    def compute_loss(self, y, y_pred, sample_weight=None):\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        return loss_fn(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_in = target[:, :-1]\n",
    "        dec_tgt = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self([source, dec_in], training=True)\n",
    "            one_hot = tf.one_hot(dec_tgt, depth=self.num_classes)\n",
    "            mask = tf.cast(tf.not_equal(dec_tgt, 0), tf.float32)\n",
    "            loss = self.compute_loss(one_hot, logits, sample_weight=mask)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        # gradient clipping already in optimizer via clipnorm\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_in = target[:, :-1]\n",
    "        dec_tgt = target[:, 1:]\n",
    "        logits = self([source, dec_in], training=False)\n",
    "        one_hot = tf.one_hot(dec_tgt, depth=self.num_classes)\n",
    "        mask = tf.cast(tf.not_equal(dec_tgt, 0), tf.float32)\n",
    "        loss = self.compute_loss(one_hot, logits, sample_weight=mask)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate_greedy(self, source, start_idx):\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source, training=False)\n",
    "        dec = tf.ones((bs, 1), dtype=tf.int32) * start_idx\n",
    "        for _ in range(self.target_maxlen - 1):\n",
    "            y = self.decode(enc, dec, training=False)\n",
    "            logits = self.classifier(y)\n",
    "            next_tok = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)[:, None]\n",
    "            dec = tf.concat([dec, next_tok], axis=1)\n",
    "        return dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84dda65",
   "metadata": {},
   "source": [
    "# Callbacks to display predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b6599de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_batch, idx_to_char, start_token_idx=START_TOKEN_IDX, end_token_idx=END_TOKEN_IDX):\n",
    "        super().__init__()\n",
    "        self.batch = sample_batch\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.start = start_token_idx\n",
    "        self.end = end_token_idx\n",
    "\n",
    "    def ids_to_text(self, ids):\n",
    "        s = []\n",
    "        for i in ids:\n",
    "            i = int(i)\n",
    "            if i == self.end:\n",
    "                break\n",
    "            ch = self.idx_to_char[i]\n",
    "            if ch in [\"<\", \">\", \"-\", \"#\"]:\n",
    "                continue\n",
    "            s.append(ch)\n",
    "        return \"\".join(s)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # print only every 5 epochs\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        batch = self.batch\n",
    "        src = batch[\"source\"]\n",
    "        tgt = batch[\"target\"].numpy()\n",
    "        preds_ids = self.model.generate_greedy(src, self.start).numpy()\n",
    "        refs_txt, preds_txt = [], []\n",
    "        for i in range(preds_ids.shape[0]):\n",
    "            rt = self.ids_to_text(tgt[i])\n",
    "            pt = self.ids_to_text(preds_ids[i])\n",
    "            refs_txt.append(rt); preds_txt.append(pt)\n",
    "            print(\"REF :\", rt)\n",
    "            print(\"PRED:\", pt)\n",
    "        cer = jiwer.cer(refs_txt, preds_txt)\n",
    "        wer = jiwer.wer(refs_txt, preds_txt)\n",
    "        print(f\"Epoch {epoch} - CER: {cer:.4f}, WER: {wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c8a45",
   "metadata": {},
   "source": [
    "# Learning rate schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec552d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, init_lr=1e-5, lr_after_warmup=5e-4, final_lr=1e-5, warmup_epochs=10, decay_epochs=3, steps_per_epoch=27):\n",
    "        super().__init__()\n",
    "        self.init_lr = float(init_lr)\n",
    "        self.lr_after_warmup = float(lr_after_warmup)\n",
    "        self.final_lr = float(final_lr)\n",
    "        self.warmup_epochs = int(warmup_epochs)\n",
    "        self.decay_epochs = int(decay_epochs)\n",
    "        self.steps_per_epoch = int(steps_per_epoch)\n",
    "\n",
    "    def calculate_lr(self, epoch):\n",
    "        warmup_lr = self.init_lr + ((self.lr_after_warmup - self.init_lr) / max(1, (self.warmup_epochs - 1))) * epoch\n",
    "        decay_lr = tf.math.maximum(self.final_lr,\n",
    "                                  self.lr_after_warmup - (epoch - self.warmup_epochs) * (self.lr_after_warmup - self.final_lr) / self.decay_epochs)\n",
    "        return tf.math.minimum(warmup_lr, decay_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step // self.steps_per_epoch\n",
    "        epoch = tf.cast(epoch, tf.float32)\n",
    "        return self.calculate_lr(epoch)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"init_lr\": self.init_lr,\n",
    "            \"lr_after_warmup\": self.lr_after_warmup,\n",
    "            \"final_lr\": self.final_lr,\n",
    "            \"warmup_epochs\": self.warmup_epochs,\n",
    "            \"decay_epochs\": self.decay_epochs,\n",
    "            \"steps_per_epoch\": self.steps_per_epoch,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20422b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - loss: 1.7692REF : so you want me to ride with you i replied yes\n",
      "PRED: and the the the the the the the an the an the the an an the the the an the the the the the the anoure the an an an\n",
      "REF : i said nothing however and after a time jane spoke the dance was one thing and riding with you is another i did not wish to dance with you but i do wish to ride with you\n",
      "PRED: and the the the the the the the an an an the the the an the the the an an the the the an the an the the the an the an an anoure the the the the an the an an the an the the the the the anore the\n",
      "REF : it meant that she cared for me and would some day be mine\n",
      "PRED: and the the the the the the the an an an the the the an the the the an the the the the the an the\n",
      "REF : this was comforting if not satisfying and loosened my tongue jane you know my heart is full of love for you\n",
      "PRED: and the the the the the the the an the the the an an an the the the an the the the the the the the the the the the an an anoure the the the the an the an an the an the the\n",
      "REF : it is wonderful what a fund of useless information some persons accumulate and cling to with a persistent determination worthy of a better cause\n",
      "PRED: and the the the the the the the the the the the the the the the the an the the the the the the the the the the the an an an the the the the the an the an an the an the the\n",
      "REF : as upon the morning of that rare ride to windsor aye surer since she knows that in all these years it has changed only to grow greater and stronger and truer\n",
      "PRED: and the the the the the the the an the the the an the the the an an an the the the the the the the the the the the an an anoure the the the the an the an the the the the the an the an an\n",
      "REF : in the fructifying light of her sweet face and the nurturing warmth of her pure soul what a blessed thing it is for a man to love his wife and be satisfied with her\n",
      "PRED: and the the the the the the the the an the the an the the the an an an the the the the the the the the the the the an an anoure the the the the the the the the the the the the the the an\n",
      "REF : who can stretch out the sweetest season of his existence\n",
      "PRED: and the the the the the the the the the the the the the the the the an the the the the the the the the the the the\n",
      "Epoch 0 - CER: 0.9933, WER: 1.7569\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5670s\u001b[0m 307ms/step - loss: 1.6588 - val_loss: 1.5713\n",
      "Epoch 2/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4940s\u001b[0m 269ms/step - loss: 1.5755 - val_loss: 1.4250\n",
      "Epoch 3/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5012s\u001b[0m 273ms/step - loss: 1.2742 - val_loss: 1.1445\n",
      "Epoch 4/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7580s\u001b[0m 414ms/step - loss: 1.1729 - val_loss: 1.0916\n",
      "Epoch 5/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7740s\u001b[0m 422ms/step - loss: 1.1232 - val_loss: 1.0498\n",
      "Epoch 6/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 1.1088REF : so you want me to ride with you i replied yes\n",
      "PRED: yes you will you will you will you will you will you will you will\n",
      "REF : i said nothing however and after a time jane spoke the dance was one thing and riding with you is another i did not wish to dance with you but i do wish to ride with you\n",
      "PRED: i don't the don't the don't don't do the don't don't don't do not the don't the don't don't the don't don't do not the don't do not the don't don't don't don't the the do the the the the the d\n",
      "REF : it meant that she cared for me and would some day be mine\n",
      "PRED: some the may man who would be some the may man who would be some\n",
      "REF : this was comforting if not satisfying and loosened my tongue jane you know my heart is full of love for you\n",
      "PRED: you know i have find the life was who long the first my find the first was who long the first my find the long the first\n",
      "REF : it is wonderful what a fund of useless information some persons accumulate and cling to with a persistent determination worthy of a better cause\n",
      "PRED: in the poor the por the poor the poor the poor the poor the poor the poor the poor the poor the poor the poor the poor the poor the por the poor the poor the portis the the the the the the the\n",
      "REF : as upon the morning of that rare ride to windsor aye surer since she knows that in all these years it has changed only to grow greater and stronger and truer\n",
      "PRED: she strick she she strange and she strange and she grated she strick she she strong she strong she strong she strange and she strong strand she grates of she stringre shishe angre gre gre gre\n",
      "REF : in the fructifying light of her sweet face and the nurturing warmth of her pure soul what a blessed thing it is for a man to love his wife and be satisfied with her\n",
      "PRED: what is a little be tree the walked the be last like the last like the last like the last of the last of the last of the last of the last of the last of the last the lasthe the the the the\n",
      "REF : who can stretch out the sweetest season of his existence\n",
      "PRED: with the can was the stort with the can was and strands with the can strance\n",
      "Epoch 5 - CER: 0.8409, WER: 1.2431\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9222s\u001b[0m 503ms/step - loss: 1.0799 - val_loss: 1.0098\n",
      "Epoch 7/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5948s\u001b[0m 324ms/step - loss: 1.0080 - val_loss: 0.9043\n",
      "Epoch 8/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7983s\u001b[0m 435ms/step - loss: 0.8147 - val_loss: 0.6923\n",
      "Epoch 9/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9493s\u001b[0m 518ms/step - loss: 0.6855 - val_loss: 0.6334\n",
      "Epoch 10/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11506s\u001b[0m 628ms/step - loss: 0.6430 - val_loss: 0.6185\n",
      "Epoch 11/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643ms/step - loss: 0.5870REF : so you want me to ride with you i replied yes\n",
      "PRED: and upon me to right would me to right wouldn't me to right wouldn't me to right\n",
      "REF : i said nothing however and after a time jane spoke the dance was one thing and riding with you is another i did not wish to dance with you but i do wish to ride with you\n",
      "PRED: i sat nothing came right with you wished to right with you wish to right with you wished to right with you wish to right with you wished to right with you wish to don't swith he was was was was was\n",
      "REF : it meant that she cared for me and would some day be mine\n",
      "PRED: it man that she cared from he unday be mind\n",
      "REF : this was comforting if not satisfying and loosened my tongue jane you know my heart is full of love for you\n",
      "PRED: this was confiding if not satisfying you might hart is for the blook fea you might hart is for the blood\n",
      "REF : it is wonderful what a fund of useless information some persons accumulate and cling to with a persistent determination worthy of a better cause\n",
      "PRED: it is one persincess to detempticumation some persincess acused to the of her battic warding to with a persinch manation words\n",
      "REF : as upon the morning of that rare ride to windsor aye surer since she knows that in all these years it has changed only to grow greater and stronger and truer\n",
      "PRED: as a pon the morning a flater rair right to great and strue winds true winds trung good godn't strung god and strung god godn't strung godn't strung godn't strung grace her said\n",
      "REF : in the fructifying light of her sweet face and the nurturing warmth of her pure soul what a blessed thing it is for a man to love his wife and be satisfied with her\n",
      "PRED: in the frocked for manted love his wide with her what her what her what her what her what her warms of the bless weet frocked find the frocked for mant love his wifted\n",
      "REF : who can stretch out the sweetest season of his existence\n",
      "PRED: who constrants season of his exist season of his existence\n",
      "Epoch 10 - CER: 0.6452, WER: 0.8674\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12154s\u001b[0m 663ms/step - loss: 0.6096 - val_loss: 0.5979\n",
      "Epoch 12/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13390s\u001b[0m 731ms/step - loss: 0.5428 - val_loss: 0.5194\n",
      "Epoch 13/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14525s\u001b[0m 793ms/step - loss: 0.4833 - val_loss: 0.4757\n",
      "Epoch 14/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15943s\u001b[0m 870ms/step - loss: 0.4498 - val_loss: 0.4465\n",
      "Epoch 15/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15374s\u001b[0m 839ms/step - loss: 0.4401 - val_loss: 0.4431\n",
      "Epoch 16/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897ms/step - loss: 0.4120REF : so you want me to ride with you i replied yes\n",
      "PRED: are pied with yes are pied\n",
      "REF : i said nothing however and after a time jane spoke the dance was one thing and riding with you is another i did not wish to dance with you but i do wish to ride with you\n",
      "PRED: that don't swas one think however i did not wished to ride with you was to ride with you was one think however i did not wished to ride with you was to ride with you was to ride with you wish to rid\n",
      "REF : it meant that she cared for me and would some day be mine\n",
      "PRED: it men littce some day be mind that she cared\n",
      "REF : this was comforting if not satisfying and loosened my tongue jane you know my heart is full of love for you\n",
      "PRED: this was confiting if not satisfying if not satisfying and loosened my heart is fine give not satisfying you know\n",
      "REF : it is wonderful what a fund of useless information some persons accumulate and cling to with a persistent determination worthy of a better cause\n",
      "PRED: it is wonderful what of fund the beautiful batter course it is wonderful what of fund the percents acumilation some persons acumilation some persons\n",
      "REF : as upon the morning of that rare ride to windsor aye surer since she knows that in all these years it has changed only to grow greater and stronger and truer\n",
      "PRED: as a pon the morning of that rarive to winds and strunger sinch in hose thanged any togrey to winds and strunger sinch in hose thanged any togrey to winds\n",
      "REF : in the fructifying light of her sweet face and the nurturing warmth of her pure soul what a blessed thing it is for a man to love his wife and be satisfied with her\n",
      "PRED: in the frocked if i anglight of us weet frace what her what her what her what her what her what her what her what her what her what her\n",
      "REF : who can stretch out the sweetest season of his existence\n",
      "PRED: who constrage out was weet istance\n",
      "Epoch 15 - CER: 0.5784, WER: 0.8232\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17648s\u001b[0m 963ms/step - loss: 0.4362 - val_loss: 0.4384\n",
      "Epoch 17/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15376s\u001b[0m 839ms/step - loss: 0.4334 - val_loss: 0.4372\n",
      "Epoch 18/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17240s\u001b[0m 941ms/step - loss: 0.4312 - val_loss: 0.4357\n",
      "Epoch 19/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17834s\u001b[0m 974ms/step - loss: 0.4294 - val_loss: 0.4367\n",
      "Epoch 20/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17754s\u001b[0m 969ms/step - loss: 0.4278 - val_loss: 0.4344\n",
      "Epoch 21/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943ms/step - loss: 0.4001REF : so you want me to ride with you i replied yes\n",
      "PRED: so you won't me to right with yes\n",
      "REF : i said nothing however and after a time jane spoke the dance was one thing and riding with you is another i did not wish to dance with you but i do wish to ride with you\n",
      "PRED: that don't swas one think and riding with you was anothing came spoke i do wish to ride with you but i do wish to ride with you but i do wish to ride with you wish announer\n",
      "REF : it meant that she cared for me and would some day be mine\n",
      "PRED: it me had would some day be mind that she cad\n",
      "REF : this was comforting if not satisfying and loosened my tongue jane you know my heart is full of love for you\n",
      "PRED: this was confiting if not satisfying if not satisfying if not satisfying if not satisfine my heart is for the blove feen\n",
      "REF : it is wonderful what a fund of useless information some persons accumulate and cling to with a persistent determination worthy of a better cause\n",
      "PRED: it is wonderful what of fund the percents acumilation some persince acumilation some persince acumilation some persince acumilation some persons acumilation some persince acumilation worthy of a bat\n",
      "REF : as upon the morning of that rare ride to windsor aye surer since she knows that in all these years it has changed only to grow greater and stronger and truer\n",
      "PRED: as a pon the morning of that raride to winds and strunger sinch in hose said to in alleasia sitter sinch in hose said to in alleasia sitter sinch\n",
      "REF : in the fructifying light of her sweet face and the nurturing warmth of her pure soul what a blessed thing it is for a man to love his wife and be satisfied with her\n",
      "PRED: in the frocked if i angland love his wife her what her what her what her what her what her what her what her what her what her\n",
      "REF : who can stretch out the sweetest season of his existence\n",
      "PRED: who constrage out was weet istance who constrage out\n",
      "Epoch 20 - CER: 0.5940, WER: 0.7514\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18341s\u001b[0m 1s/step - loss: 0.4262 - val_loss: 0.4327\n",
      "Epoch 22/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19422s\u001b[0m 1s/step - loss: 0.4247 - val_loss: 0.4320\n",
      "Epoch 23/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20858s\u001b[0m 1s/step - loss: 0.4234 - val_loss: 0.4307\n",
      "Epoch 24/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21561s\u001b[0m 1s/step - loss: 0.4222 - val_loss: 0.4306\n",
      "Epoch 25/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23555s\u001b[0m 1s/step - loss: 0.4210 - val_loss: 0.4274\n",
      "Epoch 26/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3928REF : so you want me to ride with you i replied yes\n",
      "PRED: are pied with yes are pied with yes\n",
      "REF : i said nothing however and after a time jane spoke the dance was one thing and riding with you is another i did not wish to dance with you but i do wish to ride with you\n",
      "PRED: that don't swas one think however i did not wish to ride with you but i do wish to ride with you but i do wish to ride with you but i do wish to ride with you but i do wish to ride with you\n",
      "REF : it meant that she cared for me and would some day be mine\n",
      "PRED: it me and would some day be mind that she cared\n",
      "REF : this was comforting if not satisfying and loosened my tongue jane you know my heart is full of love for you\n",
      "PRED: this was comforting if not satisfying if not satisfying if not satisfine my heart is for the blove feen\n",
      "REF : it is wonderful what a fund of useless information some persons accumulate and cling to with a persistent determination worthy of a better cause\n",
      "PRED: it is wonderful what of fund the beaution some persince acumilation some persons acumilation some persince acumilation some persons acumilation some persons\n",
      "REF : as upon the morning of that rare ride to windsor aye surer since she knows that in all these years it has changed only to grow greater and stronger and truer\n",
      "PRED: as a pon the morning of that rararier right to winds and strunger sinch in hose said to great it in alleasia sitter sinch in hose said to great it in alleasia\n",
      "REF : in the fructifying light of her sweet face and the nurturing warmth of her pure soul what a blessed thing it is for a man to love his wife and be satisfied with her\n",
      "PRED: in the frocked if i anglight of us weep face was from mantelled at his from manted love his wife her what a blessed thing it is from manted love his wife her\n",
      "REF : who can stretch out the sweetest season of his existence\n",
      "PRED: who constrage out was weet istants who constrage out\n",
      "Epoch 25 - CER: 0.5451, WER: 0.7624\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24058s\u001b[0m 1s/step - loss: 0.4197 - val_loss: 0.4278\n",
      "Epoch 27/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27826s\u001b[0m 2s/step - loss: 0.4189 - val_loss: 0.4257\n",
      "Epoch 28/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28693s\u001b[0m 2s/step - loss: 0.4177 - val_loss: 0.4242\n",
      "Epoch 29/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32100s\u001b[0m 2s/step - loss: 0.4167 - val_loss: 0.4252\n",
      "Epoch 30/30\n",
      "\u001b[1m18309/18309\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31023s\u001b[0m 2s/step - loss: 0.4157 - val_loss: 0.4233\n"
     ]
    }
   ],
   "source": [
    "steps = int(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "lr_schedule = CustomSchedule(steps_per_epoch=max(1, steps))\n",
    "\n",
    "# try AdamW; fallback to Adam if not available\n",
    "try:\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0)\n",
    "except Exception:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "\n",
    "model = TransformerASR(\n",
    "    num_hid=128, num_heads=4, num_feed_forward=512,\n",
    "    num_layers_enc=4, num_layers_dec=1, num_classes=vocab_size, dropout_rate=0.1\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# sample batch for callback\n",
    "sample_batch = next(iter(val_ds))\n",
    "display_cb = DisplayOutputs(sample_batch, idx_to_char, start_token_idx=START_TOKEN_IDX, end_token_idx=END_TOKEN_IDX)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=8, restore_best_weights=True)\n",
    "ckpt = keras.callbacks.ModelCheckpoint(\"best_asr_model.keras\", monitor='loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[display_cb, early_stop, ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47ba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
